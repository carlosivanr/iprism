---
title: "iPRISM Web Tool Data Results"
# format: docx # if output is docx, table formatting from SAS output is lost
format:
  html:
    embed-resources: true
    toc: true

#format: html

execute: 
  echo: false
  warning: false
jupyter: iprism
---

```{python}
# MAIN TABLES TO PRODUCE
# This script focuses on the analyses for the hypotheses rather than the
# overview of what is in the data
# From TableShell jy021325 - Copy_BF

# 1 3b 6 7 8 10
```

```{python, import libraries}
# Data manipulation tools
import os
import pandas as pd
import numpy as np

# Tables and plotting
from great_tables import GT, md, html, style

import matplotlib.pyplot as plt
import seaborn as sns

# Tables & statistics
import saspy
from scipy.stats import ttest_ind
from scipy.stats import spearmanr

# Custom functions
# iprism_fx found in Lib/site-packages
from iprism_fx import tables
```

```{python}
# Specify the project root directory
proj_root = 'C:\\Users\\rodrica2\\OneDrive - The University of Colorado Denver\\Documents\\DFM\\projects\\iPRISM'
```

```{python, read data}
# Specify the file to import
file1 = proj_root + "\\data\\iPRISM_entry_data_11.5.24.csv"
file2 = proj_root + "\\data\\iPRISM_entry_data_1.30.25.csv"

# Import data
df1 = pd.read_csv(file1, header = 0)
df2 = pd.read_csv(file2, header = 0)
```


```{python}
# Get
# Get the intersection of column names
common_columns = df1.columns.intersection(df2.columns)

# Get the columns in df2 using the common columns
# Select only the common columns in df1
df1 = df1[common_columns]
df2 = df2[common_columns]
```

```{python}
# COALESCE THE TWO HEADERS, THEN SET ROW 1 AS LABELS AND ROW 2 AS COLUMN NAMES
# Get the 1st row of headers as they appear in the .csv file. These will become
# the column labels for displaying
column_labels = pd.DataFrame(df1.columns.tolist(), columns = ['labels'])

# Get the 2nd row of headers since they contain shorthand names for the columns
column_names = pd.DataFrame(df1.iloc[0].tolist(), columns = ['names'])

# Create a new data frame where column names and labels are coalesced
new_column_names = pd.concat([column_names, column_labels], axis=1)

# Create a new column of the coalesced names and labels
new_column_names['coalesced'] = new_column_names.names.combine_first(new_column_names.labels)

df2.columns = new_column_names['coalesced']

# Remove the first row since it contains the headers as column names
# df1.drop(index=df.index[0], axis=0, inplace=True)

# Rename the axis column to none
# df1 = df.rename_axis(None, axis=1)

# Set df to df2
df = df2.copy()
```

```{python}
# Identify records that contain "test" in multiple columns --------------------
# List of columns to check
columns_to_check = ['progname', 'jobtitle', 'proggoal', 'popdesc']

# Create the flag_for_removal column
df['flag_for_removal'] = df[columns_to_check].apply(lambda row: (row.str.lower() == 'test').any(), axis=1).astype(int)
```

```{python}
#| eval: false

# Check the number of flags
df['flag_for_removal'].value_counts()
```

```{python}
# Identify records that contain dubious values in progname --------------------
values_to_flag = ['Cindy is the best',
'Cool cats',
'Cool stuff',
'Destroy the one ring',
'Fun times',
"Russ's fantasy",
'a',
'aa',
'asfd',
'cool stuff',
'stuff',
'ty',
'wefq',
'xyz',
'rt']

# If progname is in values to flag, set to 1 otherwise leave as is
df['flag_for_removal'] = np.where(df['progname'].isin(values_to_flag), 1, df['flag_for_removal'])
```

```{python}
#| eval: false

# Check the updated count
df['flag_for_removal'].value_counts()
```

```{python}
# test = df[df['flag'] == 1]
# test.to_csv(proj_root + "\\data\\records_to_review.csv")

# Sent to Bryan but have not heard back as of 2/6/2025
# Re-sent to Bryan and Jun on 2/26/2025
# Per Jun. remove these records

# Send out a file to Jun and Bryan 2/26/2025 on what to do with the duplicated 
# and triplicated records.
# *** This file was manually reviewed and additional test records were 
# identified. In addition, the file was manipulated in such a way were a new
# column (User ID) was created that was a copy of the Entry ID. Then, rows that
#  were suspected of having the same user, were modified so that the User ID
# was made to be the same to facilitate grouping by User ID and then slicing
# the most recent record. 2/28/2025
df.to_csv(proj_root + "\\data\\iprism_data.csv")
```

```{python}
# Specify the file to import after manual in
# This is the latest file
file = proj_root + "\\data\\iPRISM_data_BF.xlsx"

# Import data
df = pd.read_excel(file, header = 0)

# Remove those with flag for removal == 1
df = df[df["flag_for_removal"]== 0]

# Remove duplicates
# BF suggested we take the first row according to the green over red highlights
df = df.sort_values('Entry Date', ascending = True).groupby('User Id', group_keys = False).first()
```

```{python}
# Clean up the columns of the Likert variables to be able to collapse them 

# Rename exorg1 as EXPorg1. Confirmed by Bryan 12-16-2024
df['EXPorg1'] = df['exorg1']

prefixes = [
'Aper',
'Aeq',
'Astper',
'Astint',
'Astdec',
'Ifid',
'Iadapt',
'Icost',
'Rper',
'Req',
'E',
'Eeq',
'Mper',
'Madapt',
'Meff',
'Meffeq',
'EXPcom',
'EXPorg',
'CHcom',
'CHorg',
'res',
'CHexenv']

# For each prefix, select the columns that have the 1, 2, or 3 suffix then 
# coalesce/backwards fill to a new column that is then appended horizontally
for prefix in prefixes:
  # print(prefix)
  temp = df[[prefix + '1', prefix + '2', prefix + '3']].copy()
  col_name = prefix
  temp[col_name] = pd.to_numeric(temp.bfill(axis=1).iloc[:,0])
  df = pd.concat([df, temp[col_name]], axis = 1)

  # *** Not all prefixes may have 3 variables
```



## Table 1 - Summary statistics of PRISM and RE-AIM measure
```{python}
# Create a table of the prism and re-aim values only
tab1 = df[prefixes].copy()
```

```{python}
# Create a composite of Iadap Icost Ifid
tab1["Icomp"] = tab1[["Iadapt", "Icost", "Ifid"]].mean(axis=1)
```

```{python}
# Set the order the columns to display prism then re-aim measures
order = [
  "EXPcom",
  "EXPorg",
  "CHcom",
  "CHorg",
  "res",
  "CHexenv",
  "Rper",
  "E",
  "Aper",
  "Icomp",
  "Iadapt",
  "Icost",
  "Ifid",
  "Mper",
  "Req",
  "Eeq",
  "Aeq",
  "Meffeq"
]

# Reorder the tab1 data frame
tab1 = tab1[order]
```

```{python}
# Rename columns to display in a table
col_name_dict = {
  "Aper": "Adoption", 
  "Aeq": "Adoption Equity",
  'Astper': "Adoption Staff",
  'Astint': "Adoption Intended",
  'Astdec': "Adoption Declined",
  'Ifid': "Implementation Fidelity",
  'Iadapt': "Implementation Adapt",
  'Icost': "Implementation Cost",
  'Icomp': "Implementation Composite",
  'Rper': "Reach",
  'Req': "Reach Equity",
  'E': "Effectiveness",
  'Eeq': "Effectiveness Equity",
  'Mper': "Maintenance",
  'Madapt': "Maintenance Adaptation",
  'Meff': "Maintenance Effectiveness",
  'Meffeq': "Maintenance Effectiveness Equity",
  'EXPcom': "Expectation of Recipients",
  'EXPorg': "Expectation of Organization",
  'CHcom': "Characteristics of Recipients",
  'CHorg': "Characteristics of Organization",
  'res': "Implementation Sustainability Infrastructure",
  'CHexenv': "External Environment"
}

tab1 = tab1.rename(columns=col_name_dict)
```

```{python}
# Create a summary dataframe
summary_tab1 = pd.DataFrame({
    "N": tab1.count(),
    "Mean": tab1.mean().round(2),
    "StdDev": tab1.std().round(2),
    "Median": tab1.median().round(2),
    "Min": tab1.min(),
    "Max": tab1.max()
}).reset_index()

# Rename the index column to something descriptive
summary_tab1.rename(columns={"index": "Measure"}, inplace=True)

# Display the table
GT(summary_tab1)
```


## Table 2 - PRISM and RE-AIM Measures by Setting Type
```{python}
# Table 2 refers to table 3b in the TableShell file
# Mean +- SE for each variable and P-value

# Create a new data frame
tab2 = df[["Community", "Clinical", "Public health", "Other.1"]].copy()  #.mean(axis=1)

# Rename the Other.1 column to other
tab2.rename(columns={"Other.1": "Other"}, inplace=True)

# Convert all values to binary
tab2 = tab2.notna().astype(int)

# Get a row sum to make a new variable called multiple that indicates that the
# participant selected multiple settings
tab2["Multiple"] = tab2.sum(axis = 1)

tab2["Multiple"] = np.where(tab2["Multiple"] > 0, 1, tab2["Multiple"])

# Merge the two data frames to conduct t-tests
tab2 = pd.concat([tab2, tab1], axis = 1)
```

```{python}
# Define a function to format p-values ----------------------------------------
# If the p value is less than <0.001 then print it as character rather than
# printing the entire numeric value
def format_p_value(p):
    # if isinstance(p, np.ndarray):
    #     p = p.item()  # Extracts the first element as a float

    if p < 0.0001:
        return "<0.0001"
    else:
        return f"{p:.2f}"

```

```{python}
# Create a dictionary where each element is a separate data frame -------------
# Each data frame  has all of the re-aim measures broken down by a specific
# setting type
 
# Placeholder for results grouped by IV
results_by_iv = {}

# Set the dependent variables from the names of the columns in tab1
dependent_vars = tab1.columns.tolist()

# Set the independent variables for tab2
independent_vars = ["Community", "Clinical", "Public health", "Other", "Multiple"]

for iv in independent_vars:
    results = []

    for dv in dependent_vars:
        # Check unique groups in the IV
        groups = tab2[iv].unique()
        if len(groups) != 2:
            raise ValueError(f"Expected exactly two groups in {iv}, found {len(groups)}")

        # Split data into the two groups based on IV
        group_1 = tab2.loc[tab2[iv] == groups[1], dv]
        group_2 = tab2.loc[tab2[iv] == groups[0], dv]

        # Compute means and standard errors
        group_1_mean = group_1.mean().round(2)
        group_2_mean = group_2.mean().round(2)
        group_1_se = group_1.sem().round(2)
        group_2_se = group_2.sem().round(2)

        # Perform t-test
        t_stat, p_val = ttest_ind(group_1, group_2, equal_var=False)

        # Modify the p-value
        p_val = format_p_value(p_val)
      
        # Store the results within a specific iv
        results.append({
          "Dependent Variable": dv,
          "Independent Variable": iv,
          "Yes Mean (SE)": f"{group_1_mean}, ({group_1_se})",
          "No Mean (SE)": f"{group_2_mean}, ({group_2_se})",
          "p-value": p_val
        })

    # Convert results to a data frame
    results_by_iv[iv] = pd.DataFrame(results)


```

```{python}
# Could save results_by_iv as a spread sheet here
```

```{python}
# For each element in results_by_iv, modify the table and display -------------
for iv in independent_vars:
    # print(results_by_iv[iv])
    sub_tab = results_by_iv[iv].drop(columns=["Independent Variable"])
    sub_tab = GT(sub_tab).tab_spanner(
      label = md("**" + iv +"**"),
      columns = ["Yes Mean (SE)", "No Mean (SE)", "p-value"])

    display(sub_tab)
    print("\n") # Try printing a new line for .docx
    # cat("```{=openxml}", "```", sep = "\n")

```

## Table 3 - Correlation between each RE-AIM measure and each PRISM measure
```{python}
# Table 6 in the TableShell file
# Correlation between PRISM measures and RE-AIM measures including equity measures

reach = ["Reach", "Effectiveness", "Adoption", "Implementation Composite", 
"Maintenance", "Reach Equity", "Effectiveness Equity", "Adoption Equity", 
"Maintenance Effectiveness Equity"]

prism = ["Expectation of Recipients", "Expectation of Organization", 
"Characteristics of Recipients", "Characteristics of Organization", 
"Implementation Sustainability Infrastructure", "External Environment"]

# Empty list to store results
correlation_results = []

for r_var in reach:
    for p_var in prism:
        # Compute Spearman correlation
        r_value, p_value = spearmanr(tab1[r_var], tab1[p_var])
        
        # Modify the p-value
        p_value = format_p_value(p_value)

        # Append results as a dictionary
        correlation_results.append({
            "Reach Variable": r_var,
            "PRISM Variable": p_var,
            "Spearman r-value": r_value.round(2),
            "p-value": p_value
        })


# GT(pd.DataFrame(correlation_results))
```

```{python}
# Create a correlation matrix--------------------------------------------------
cor_tab = tab1[reach + prism].corr(method = "spearman")

# Select only the prism columns
cor_tab = cor_tab[prism]

# Select only the reach rows
cor_tab = cor_tab.loc[reach]

# Display the correlation matrix
cor_tab.round(3)
```

```{python}
# eval: false
# A way to print the correlations as a GT table
# The rounding lookds good, but it looks better in .html because the bold
# headers are maintained
# GT(cor_tab.round(2).reset_index().rename(columns={'index': ''}))
```

```{python}
# Create a p value matrix
# Take the p-values and reshape them into a matrix
p_mat = pd.DataFrame(correlation_results)["p-value"].to_numpy().reshape(9, 6)

# Convert back to data frame
p_mat = pd.DataFrame(p_mat)

# Rename the index (rows)
p_mat.index = reach

# Rename the columns
p_mat.columns = prism

# Display the pvalue matrix
p_mat
```

## Table 4 - Correlation between each RE-AIM measure and each equity RE-AIM measure
```{python}
# Table 7 in the TableShell file
# Correlation between RE-AIM and RE-AIM equity measures
reach = ["Reach", "Effectiveness", "Adoption", "Implementation Composite", 
"Maintenance"]

equity = ["Reach Equity", "Effectiveness Equity", "Adoption Equity", 
"Maintenance Effectiveness Equity"]


# Empty list to store results
eq_corr_results = []

for r_var in reach:
    for eq_var in equity:
        # Compute Spearman correlation
        r_value, p_value = spearmanr(tab1[r_var], tab1[eq_var])
        
        # Modify the p-value
        p_value = format_p_value(p_value)

        # Append results as a dictionary
        eq_corr_results.append({
            "Reach Variable": r_var,
            "Equity Variable": eq_var,
            "Spearman r-value": r_value.round(2),
            "p-value": p_value
        })

# GT(pd.DataFrame(eq_corr_results))
```


```{python}
# Create a correlation matrix--------------------------------------------------
cor_tab = tab1[reach + equity].corr(method = "spearman")

# Select only the equity columns
cor_tab = cor_tab[equity]

# Select only the reach rows
cor_tab = cor_tab.loc[reach]

# Display the correlation matrix
cor_tab.round(3)
```

```{python}
# Create a p value matrix
# Take the p-values and reshape them into a matrix
p_mat = pd.DataFrame(eq_corr_results)["p-value"].to_numpy().reshape(5, 4)

# Convert back to data frame
p_mat = pd.DataFrame(p_mat)

# Rename the index (rows)
p_mat.index = reach

# Rename the columns
p_mat.columns = equity

# Display the pvalue matrix
p_mat
```

## Table 5 - Correlations between each RE-AIM measure
```{python}
# Correlations between RE-AIM measures and RE-AIM measures 
reach = ["Reach", "Effectiveness", "Adoption", "Implementation Composite", 
"Maintenance"]

# Empty list to store results
reach_corr_results = []

for r_var in reach:
    for r_var2 in reach:
        # Compute Spearman correlation
        r_value, p_value = spearmanr(tab1[r_var], tab1[r_var2])
        
        # Modify the p-value
        p_value = format_p_value(p_value)

        # Append results as a dictionary
        reach_corr_results.append({
            "Reach Variable1": r_var,
            "Reach Variable2": r_var2,
            "Spearman r-value": r_value.round(2),
            "p-value": p_value
        })

# GT(pd.DataFrame(reach_corr_results))
```

```{python}
# Compute Spearman correlation matrix
reach_corr_matrix = tab1[reach].corr(method='spearman')
reach_corr_matrix.round(3)
```

```{python}
# Take the p-values and reshape them into a matrix
reach_p_matrix = pd.DataFrame(reach_corr_results)["p-value"].to_numpy().reshape(5, 5)

# Fill the diagonal with 0s
np.fill_diagonal(reach_p_matrix, " ")

# Convert back to data frame
reach_p_matrix = pd.DataFrame(reach_p_matrix)

# Rename the index
reach_p_matrix.index = reach

# Rename the columns
reach_p_matrix.columns = reach

# Display the pvalue matrix
reach_p_matrix
```

## Table 6 - Correlations between each PRISM measure
```{python}
# Correlations between PRISM measure

# Empty list to store results
prism_corr_results = []

for p_var in prism:
    for p_var2 in prism:
        # Compute Spearman correlation
        r_value, p_value = spearmanr(tab1[p_var], tab1[p_var2])
        
        # Modify the p-value
        p_value = format_p_value(p_value)

        # Append results as a dictionary
        prism_corr_results.append({
            "Reach Variable1": p_var,
            "Reach Variable2": p_var2,
            "Spearman r-value": r_value.round(2),
            "p-value": p_value
        })

# GT(pd.DataFrame(prism_corr_results))
```

```{python}
# Compute Spearman correlation matrix
prism_corr_matrix = tab1[prism].corr(method='spearman')
prism_corr_matrix.round(3)
```

```{python}
# Take the p-values and reshape them into a matrix
prism_p_matrix = pd.DataFrame(prism_corr_results)["p-value"].to_numpy().reshape(6, 6)

# Fill the diagonal with 0s
np.fill_diagonal(prism_p_matrix, " ")

# Convert back to data frame
prism_p_matrix = pd.DataFrame(prism_p_matrix)

# Rename the index
prism_p_matrix.index = prism

# Rename the columns
prism_p_matrix.columns = prism

# Display the pvalue matrix
prism_p_matrix
```

# Table 7 - Intraclass correlation coefficients for each measure
- Intraclass correlation coefficients calculated for those with two or more entries per team.
- Data available for 22 teams
- Data available for 127 participants
```{python}
# Intraclass correlation within team members
# Needs work to consolidate team-members

icc_data = df.copy()

# Drop missing values in teamcode
icc_data = icc_data.dropna(subset=['teamcode'])

# Convert all values to lower case
icc_data['teamcode'] = icc_data['teamcode'].str.lower()

# strip away blanks
icc_data['teamcode'] = icc_data['teamcode'].str.replace(" ", "")

# Check values here

# then find out who has more than one value per group
teams = icc_data.groupby('teamcode').size().reset_index(name = "count").query("count > 1")

# Number of rows
teams['count'].sum()

# filter the icc data
icc_data = icc_data[icc_data['teamcode'].isin(teams['teamcode'])]

# Create a composite of Iadap Icost Ifid
icc_data["Icomp"] = icc_data[["Iadapt", "Icost", "Ifid"]].mean(axis=1)

# Rename columns
icc_data = icc_data.rename(columns=col_name_dict)
```

```{python}
#| eval: false
# import pingouin as pg
import statsmodels.api as sm
import statsmodels.formula.api as smf


# Stats models
model = smf.mixedlm("Reach ~ 1", icc_data, groups=icc_data["teamcode"]).fit()

# Extract variance components
var_team = model.cov_re.iloc[0, 0]  # Between-group (team) variance
var_residual = model.scale  # Within-group variance

# Compute ICC
icc = var_team / (var_team + var_residual)
print(f"ICC: {icc:.4f}")
```

```{python}
#| eval: false
# Start a SAS session to connect python and SAS
# Use verbose = False to prevent printing the session ID to console
sas = saspy.SASsession(cfgname = 'autogen_winlocal', verbose = False);
```

```{python}
#| eval: false

sas_data = sas.df2sd(icc_data[['teamcode', 'Reach']], verbose = False)

# cols = pd.DataFrame(df.drop(columns=prefixes).columns)

# submit to sas and display
c = sas.submitLST("""
ods on;

*ods output CovParms = covp;

proc mixed data = work._df covtest;
  class teamcode;
  model reach = /s;
  random intercept/subject = teamcode;
run;

data icc;
  set covp end=last;
  retain bvar;
  if subject~="" then bvar = estimate;
  if last then icc = bvar/(bvar+estimate);
run;

proc print data = icc;
run;

""")

# Define the SAS dataset
sas_dataset = sas.sasdata("icc", libref="work")  # Change dataset and library names accordingly

# Convert the SAS dataset to a Pandas DataFrame
sas_icc = sas_dataset.to_df()

sas_icc
```


```{python}
#| echo: false
# Set up a sas session
sas = saspy.SASsession(cfgname = 'autogen_winlocal', verbose = False);

# Prepare an empty list to store results
icc_results = pd.DataFrame()

# Combine all of the prism/re-aim variables
all_variables = prism + reach + equity

# Loop through each measure column
for measure in all_variables:  # Add more measures as needed
    # Create a data frame where only the teamcode and the measure are present
    df_to_sas = icc_data.copy()
    df_to_sas = df_to_sas[['teamcode', measure]].reset_index(drop = True)

    # rename the Prism/Re-AIM measure to "measure"
    df_to_sas["measure"] = df_to_sas[measure]

    # Drop the column otherwise the length of the name is too long for SAS
    df_to_sas.drop(columns=[measure], inplace=True)

    # SAS section begin -------------------------------------------------------
    # Send the sas_df to SAS
    sas_data = sas.df2sd(df_to_sas, verbose = False)

    # Submit SAS commands
    c = sas.submit("""
    ods select none;
    ods output CovParms = covp;
    
    proc mixed data = work._df covtest;
      class teamcode;
      model measure = /s;
      random intercept/subject = teamcode;
    run;

    data icc;
      set covp end=last;
      retain bvar;
      if subject~="" then bvar = estimate;
      if last then icc = bvar/(bvar+estimate);
    run;

    *proc print data = icc;
    *run;


    """)

    # Retrieve output from SAS. First, define a dataset by mappping the 
    # SAS datasetname and library to a saspy object
    sas_to_pd = sas.sasdata("icc", libref="work")

    # Convert the SAS dataset to a Pandas DataFrame
    sas_icc = sas_to_pd.to_df()

    # Select the row and columns of interest 
    sas_icc = sas_icc[["icc", "ProbZ"]].iloc[[1]]

    # Reset the index to the variable measure
    sas_icc.index = [measure]

    # Format the p value
    p = format_p_value(sas_icc.loc[:, 'ProbZ'].values[0])
    sas_icc["ProbZ"] = p

    # Format the ICCs
    sas_icc['icc'] = sas_icc['icc'].round(2)

    # rename ProbZ to p-value
    sas_icc.rename(columns={"ProbZ": 'p-value'}, inplace=True)

    icc_results = pd.concat([icc_results, sas_icc], axis=0)


```

```{python}
# Display the ICC results
icc_results
```


<!-- ******************************** FENCE BEGIN ************************* -->

# Table 8 - Comparisons between stages

```{python}
# Create Icomp1, Icomp2, Icomp3
df['Icomp1'] = df[['Iadapt1', 'Icost1', 'Ifid1']].mean(axis = 1, skipna = True)

df['Icomp2'] = df[['Iadapt2', 'Icost2', 'Ifid2']].mean(axis = 1, skipna = True)

df['Icomp3'] = df[['Iadapt3', 'Icost3', 'Ifid3']].mean(axis = 1, skipna = True)

# Add the Icomp to the prefixes
prefixes = prefixes + ['Icomp']
```

```{python}
# Redefine the format_p_value function
def format_p_value(p):
    if isinstance(p, np.ndarray):
        p = p.item()  # Extracts the first element as a float

    if p < 0.0001:
        return "<0.0001"
    else:
        return f"{p:.2f}"

# Create an empty data frame to hold results
pairwise_results = pd.DataFrame()

for prefix in prefixes:
    # print(prefix)
    temp = df[[prefix + '1', prefix + '2', prefix + '3']].copy()
    
    # Create a table of means
    means = pd.DataFrame(temp.mean(axis=0, skipna=True)).round(3).T
    means = means.rename(columns={prefix + '1': "(1) Planning",
                          prefix + '2': "(2) Implementing",
                          prefix + '3': "(3) Sustaining"})


    # Comparison 1
    tstat1, pval1 = ttest_ind(temp[[prefix + '1']], temp[[prefix + '2']], nan_policy='omit', equal_var=False)

    # Modify the p-value
    pval1 = format_p_value(pval1)

    # Comparison 2
    tstat2, pval2 = ttest_ind(temp[[prefix + '1']], temp[[prefix + '3']], nan_policy='omit', equal_var=False)

    # Modify the p-value
    pval2 = format_p_value(pval2)

    # Comparison 3
    tstat3, pval3 = ttest_ind(temp[[prefix + '2']], temp[[prefix + '3']], nan_policy='omit', equal_var=False)

    # Modify the p-value
    pval3 = format_p_value(pval3)


    # Store results in a data frame
    results = pd.DataFrame([{"(1) vs (2)": pval1,
    "(1) vs (3)": pval2,
    "(2) vs (3)": pval3}])

    # join the pairwise comparisons
    pairwise = pd.concat([means, results], axis = 1)
    pairwise.index = [prefix]

    pairwise_results = pd.concat([pairwise_results, pairwise], axis = 0)


```

```{python}
# reorder the indexes
pairwise_results = pairwise_results.loc[order]
```

```{python}
# Reorder the dictionary according to the list order
# ordered_dict = {key: col_name_dict[key] for key in order}

# Rename the indexes
pairwise_results = pairwise_results.rename(index=col_name_dict)
```

```{python}
# Display the table
pairwise_results
```
<!-- ******************************** FENCE END *************************** -->

---
title: "iPRISM Web Tool Data Results"
# format: docx # if output is docx, table formatting from SAS output is lost
format:
  html:
    embed-resources: true
    toc: true
    number-sections: true


#format: html

execute: 
  echo: false
  warning: false
jupyter: iprism
---

```{python}
#| eval: false
# -----------------------------------------------------------------------------
# Carlos Rodriguez, PhD. CU Anschutz Dept. of Family Medicine
# The following report relies on visually inspected data to produce a report
# displaying the analyses outlined in the statistical analysis plan.
# Relies on custom functions in the 
# [dfm_tb](https://github.com/carlosivanr/dfm_tb) package 
# -----------------------------------------------------------------------------
```

```{python, import libraries}
# Data manipulation tools
import os
import pandas as pd
import numpy as np


# Tables and plotting
from great_tables import GT, md, html, style

# Plotting
import matplotlib.pyplot as plt
import seaborn as sns

# Tables & statistics
import saspy
from scipy.stats import ttest_ind
from scipy.stats import spearmanr
from scipy.stats import norm

# Custom functions
from dfm_tb.utilities import format_pval_df
from dfm_tb.utilities import format_pval_flt
```

```{python}
# Specify the project root directory
proj_root = 'C:\\Users\\rodrica2\\OneDrive - The University of Colorado Denver\\Documents\\DFM\\projects\\iPRISM'
```

```{python}
# Specify the file to import after manual inspection/manipulation
file = proj_root + "\\data\\iPRISM_data_BF.xlsx"

# Import data
df = pd.read_excel(file, header = 0)

# Remove those with flag for removal == 1
df = df[df["flag_for_removal"]== 0]

# Remove duplicates
# BF suggested we take the first row according to the green over red highlights
df = df.sort_values('Entry Date', ascending = True).groupby('User Id', group_keys = False).first()

# *** Address the issue of the 4 participants that filled out planning,
# implementing, and sustaining questions
# Option 1: remove the rows
# df = df.drop(index=[508, 509, 512, 514])

# Option 2: Keep only the responses according to their selected impstage
# If impstage == "planning", set any prefix that ends in 2 or 3 to NA
# If impstage == "implementing", set any prefix that ends in 1 or 3 to NA
# implemented downstream
```

```{python}
# Clean up the columns of the Likert variables to be able to collapse them

# Rename exorg1 as EXPorg1. Confirmed by Bryan 12-16-2024
df.rename(columns={"exorg1": "EXPorg1"}, inplace=True)

prefixes = [
'Aper',
'Aeq',
'Astper',
'Astint',
'Astdec',
'Ifid',
'Iadapt',
'Icost',
'Rper',
'Req',
'E',
'Eeq',
'Mper',
'Madapt',
'Meff',
'Meffeq',
'EXPcom',
'EXPorg',
'CHcom',
'CHorg',
'res',
'CHexenv']
```

```{python}
# Option 2: For addressing responses in multiple stages
# 4 participants entered data for all time points
# Comment out this section if needed to revert to original algorithm of the 
# first non-missing value available.
rows_to_modify = [508, 509, 512, 514]

for row in rows_to_modify:
    if df.loc[row, 'impstage'] == "planning":
        cols_to_modify = [col for col in df.columns if any(col.startswith(prefix) for prefix in prefixes) and col[-1] in {'2', '3'}]
        df.loc[row, cols_to_modify] = np.nan  # Set matching columns to NaN

    elif df.loc[row, 'impstage'] == "implementing":  # Use `elif` instead of `else`
        cols_to_modify = [col for col in df.columns if any(col.startswith(prefix) for prefix in prefixes) and col[-1] in {'1', '3'}]
        df.loc[row, cols_to_modify] = np.nan  # Set matching columns to NaN

```

```{python}
# 0s in the likert items represent NAs convert before coalescing

for prefix in prefixes:
  temp = df[[prefix + '1', prefix + '2', prefix + '3']].copy()
  temp = temp.replace(0, np.nan)
  df[[prefix + '1', prefix + '2', prefix + '3']] = temp
```

```{python}
# For each prefix, select the columns that have the 1, 2, or 3 suffix then 
# coalesce/backwards fill to a new column that is then appended horizontally
for prefix in prefixes:
  # print(prefix)
  temp = df[[prefix + '1', prefix + '2', prefix + '3']].copy()
  col_name = prefix
  temp[col_name] = pd.to_numeric(temp.bfill(axis=1).iloc[:,0])
  df = pd.concat([df, temp[col_name]], axis = 1)

```


# Summary statistics of PRISM and RE-AIM measures
```{python}
# Create a table of the prism and re-aim values only
# There are no missing values, because each measure is coalesced.
tab1 = df[prefixes].copy()


```

```{python}
# Create a composite of Iadap Icost Ifid
# Currently, a NaN will arise in Icomp only if all Implementation measures
# are missing. If at least one is present, then a mean will be calculated
tab1["Icomp"] = tab1[["Iadapt", "Icost", "Ifid"]].mean(axis=1)

# Count non-missing values in columns A, B, and C row-wise
tab1['N_non_missing_Icomp'] = tab1[["Iadapt", "Icost", "Ifid"]].notna().sum(axis=1).astype(int)

# For Icomp, two or more Implementation variables are needed to construct a Composite
# Set Icomp to missing if only one Implemenation variable is present
tab1.loc[tab1['N_non_missing_Icomp']  <2, 'Icomp'] = np.nan

# Remove the N_non_missing_Icomp column
tab1 = tab1.drop('N_non_missing_Icomp', axis=1)

# output the items only data set to to .csv for comparisons of correlations
tab1.to_csv(proj_root + "\\data\\corr_data.csv")
```

```{python}
# Set the order the columns to display prism then re-aim measures
order = [
  "EXPcom",
  "EXPorg",
  "CHcom",
  "CHorg",
  "res",
  "CHexenv",
  "Rper",
  "E",
  "Aper",
  "Icomp",
  "Iadapt",
  "Icost",
  "Ifid",
  "Mper",
  "Req",
  "Eeq",
  "Aeq",
  "Meffeq"
]

# Reorder the tab1 data frame
tab1 = tab1[order]
```

```{python}
# Rename columns to display in a table
col_name_dict = {
  "Aper": "Adoption", 
  "Aeq": "Adoption Equity",
  'Astper': "Adoption Staff",
  'Astint': "Adoption Intended",
  'Astdec': "Adoption Declined",
  'Ifid': "Implementation Fidelity",
  'Iadapt': "Implementation Adapt",
  'Icost': "Implementation Cost",
  'Icomp': "Implementation Composite",
  'Rper': "Reach",
  'Req': "Reach Equity",
  'E': "Effectiveness",
  'Eeq': "Effectiveness Equity",
  'Mper': "Maintenance",
  'Madapt': "Maintenance Adaptation",
  'Meff': "Maintenance Effectiveness",
  'Meffeq': "Maintenance Effectiveness Equity",
  'EXPcom': "Expectation of Recipients",
  'EXPorg': "Expectation of Organization",
  'CHcom': "Characteristics of Recipients",
  'CHorg': "Characteristics of Organization",
  'res': "Implementation Sustainability Infrastructure",
  'CHexenv': "External Environment"
}

# Make a copy of the prefixed columns for correlations
prefixed_df = tab1.copy()

# Rename the columns
tab1 = tab1.rename(columns=col_name_dict)
```

```{python}
# Create a summary dataframe
summary_tab1 = pd.DataFrame({
    "N": tab1.count(),
    "Missing": tab1.isna().sum(),
    "% missing": ((tab1.isna().sum() / tab1.count()) * 100). round(2),
    "Mean": tab1.mean().round(2),
    "StdDev": tab1.std().round(2),
    "Median": tab1.median().round(2),
    "Min": tab1.min(),
    "Max": tab1.max()
}).reset_index()

# Rename the index column to "Measure"
summary_tab1.rename(columns={"index": "Measure"}, inplace=True)

# Display the table
GT(summary_tab1)
```


# PRISM and RE-AIM Measures by Setting Type
```{python}
# Mean +- SE for each variable and P-value

# Create a new data frame, that will be the result of concatenating tab1 and
# a new data frame with the columns containing the Setting Type values.
tab2 = df[["Community", "Clinical", "Public health", "Other.1"]].copy()  #.mean(axis=1)

# Rename the Other.1 column to "Other", Other.1 arises because there are two
# columns name Other
tab2.rename(columns={"Other.1": "Other"}, inplace=True)

# Convert all values to binary
tab2 = tab2.notna().astype(int)

# Get a row sum to make a new variable called "Multiple" that indicates that the
# participant selected multiple settings
tab2["Multiple"] = tab2.sum(axis = 1)

# Since Multiple is a sum, set to 1 where the value is greater than 1, else set
# to 0
tab2["Multiple"] = np.where(tab2["Multiple"] > 1, 1, 0)

# Merge the two data frames to conduct t-tests
tab2 = pd.concat([tab2, tab1], axis = 1)
```

```{python}
# Create a dictionary where each element is a separate data frame -------------
# Each data frame  has all of the re-aim measures broken down by a specific
# setting type
 
# Placeholder for results grouped by IV
results_by_iv = {}

# Set the dependent variables from the names of the columns in tab1
# These will correspond to all of the REAIM/PRISM measures
dependent_vars = tab1.columns.tolist()

# Set the independent variables for tab2
independent_vars = ["Community", "Clinical", "Public health", "Other", "Multiple"]

for iv in independent_vars:
    results = []

    for dv in dependent_vars:
        # Check unique groups in the IV to ensure 2 and only 2 groups are available
        groups = tab2[iv].unique()
        if len(groups) != 2:
            raise ValueError(f"Expected exactly two groups in {iv}, found {len(groups)}")

        # Split data into the two groups based on IV
        group_1 = tab2.loc[tab2[iv] == groups[1], dv].dropna()
        group_2 = tab2.loc[tab2[iv] == groups[0], dv].dropna()

        # Compute means and standard errors
        group_1_n = len(group_1)
        group_2_n = len(group_2)
        group_1_mean = group_1.mean().round(2)
        group_2_mean = group_2.mean().round(2)
        group_1_se = group_1.sem().round(2)
        group_2_se = group_2.sem().round(2)

        # Perform t-test
        # equal_var = False gives Welch's t-test
        t_stat, p_val = ttest_ind(group_1, group_2, nan_policy = "omit", equal_var=False)

        # Modify the p-value, setting the number of digits
        p_val = format_pval_flt(p_val, 4)
      
        # Store the results within a specific iv
        results.append({
          "Dependent Variable": dv,
          "Independent Variable": iv,
          "Yes: N": f"{group_1_n}",
          "Yes: Mean (SE)": f"{group_1_mean} ({group_1_se})",
          "No: N": f"{group_2_n}",
          "No: Mean (SE)": f"{group_2_mean} ({group_2_se})",
          "p-value": p_val
        })

    # Convert results to a data frame
    results_by_iv[iv] = pd.DataFrame(results)


```

```{python}
# For each element in results_by_iv, modify the table and display -------------
for iv in independent_vars:
    # print(results_by_iv[iv])
    n_yes = results_by_iv[iv]["Yes: N"][0]
    n_no = results_by_iv[iv]["No: N"][0]
    sub_tab = results_by_iv[iv].drop(columns=["Independent Variable", "Yes: N", "No: N"])
    
    # Rename the columns to include the sample size for each t-test
    sub_tab.rename(
        columns={"Yes: Mean (SE)": f"Yes: N = {n_yes}, Mean (SE)", 
                 "No: Mean (SE)": f"No: N = {n_no}, Mean (SE)"}, 
        inplace=True)

    # Get the column names to index them dynamically within the loop
    col_1 = sub_tab.columns[1]
    col_2 = sub_tab.columns[2]

    # Add the table spanner to indicate which community setting
    sub_tab = GT(sub_tab).tab_spanner(
      label = md("**" + iv +"**"),
      columns = [col_1, col_2, "p-value"])

    display(sub_tab)
    print("\n") # Try printing a new line for .docx
    # cat("```{=openxml}", "```", sep = "\n")

```


# PRISM and RE-AIM Measures by Role
```{python}
# Mean +- SE for each variable and P-value

# Create a new data frame with the columns of interest
tab3 = df[["Clinician", "Quality improvement specialist", "Researcher", "Public health practitioner",
"Program manager", "Implementation specialist", "Clinical administrator (e.g., Medical director)",
"Academic administrator (e.g., Department chair, dean)", "Other"]].copy()  #.mean(axis=1)

# Convert all values to binary
tab3 = tab3.notna().astype(int)

# Get a row sum to make a new variable called multiple that indicates that the
# participant selected multiple roles
tab3["Multiple"] = tab3.sum(axis = 1)

# Since Multiple is a sum, set to 1 where the value is greater than 1, else set
# to 0
tab3["Multiple"] = np.where(tab3["Multiple"] > 1, 1, 0)

# Merge the two data frames to conduct t-tests
tab3 = pd.concat([tab3, tab1], axis = 1)
```

```{python}
# Create a dictionary where each element is a separate data frame -------------
# Each data frame has all of the re-aim measures broken down by a specific
# role
 
# Placeholder for results grouped by IV
results_by_iv = {}

# Set the dependent variables from the names of the columns in tab1
# These will correspond to all of the REAIM/PRISM measures
dependent_vars = tab1.columns.tolist()

# Set the independent variables for tab2
independent_vars = ["Clinician", "Quality improvement specialist", "Researcher",
"Public health practitioner", "Program manager", "Implementation specialist",
"Clinical administrator (e.g., Medical director)", 
"Academic administrator (e.g., Department chair, dean)", "Other", "Multiple"]

for iv in independent_vars:
    results = []

    for dv in dependent_vars:
        # Check unique groups in the IV
        groups = tab3[iv].unique()
        if len(groups) != 2:
            raise ValueError(f"Expected exactly two groups in {iv}, found {len(groups)}")

        # Split data into the two groups based on IV
        group_1 = tab3.loc[tab3[iv] == groups[1], dv].dropna()
        group_2 = tab3.loc[tab3[iv] == groups[0], dv].dropna()

        # Compute means and standard errors
        group_1_n = len(group_1)
        group_2_n = len(group_2)
        group_1_mean = group_1.mean().round(2)
        group_2_mean = group_2.mean().round(2)
        group_1_se = group_1.sem().round(2)
        group_2_se = group_2.sem().round(2)

        # Perform t-test
        # equal_var = False gives Welch's t-test
        t_stat, p_val = ttest_ind(group_1, group_2, nan_policy='omit', equal_var=False)

        # Modify the p-value
        p_val = format_pval_flt(p_val, 4)
      
        # Store the results within a specific iv
        results.append({
          "Dependent Variable": dv,
          "Independent Variable": iv,
          "Yes: N": f"{group_1_n}",
          "Yes: Mean (SE)": f"{group_1_mean} ({group_1_se})",
          "No: N": f"{group_2_n}",
          "No: Mean (SE)": f"{group_2_mean} ({group_2_se})",
          "p-value": p_val
        })

    # Convert results to a data frame
    results_by_iv[iv] = pd.DataFrame(results)


```

```{python}
# For each element in results_by_iv, modify the table and display -------------
for iv in independent_vars:
    # print(results_by_iv[iv])
    n_yes = results_by_iv[iv]["Yes: N"][0]
    n_no = results_by_iv[iv]["No: N"][0]
    sub_tab = results_by_iv[iv].drop(columns=["Independent Variable", "Yes: N", "No: N"])
    
    # Rename the columns to include the sample size for each t-test
    sub_tab.rename(
        columns={"Yes: Mean (SE)": f"Yes: N = {n_yes}, Mean (SE)", 
                 "No: Mean (SE)": f"No: N = {n_no}, Mean (SE)"}, 
        inplace=True)

    # Get the column names to index them dynamically within the loop
    col_1 = sub_tab.columns[1]
    col_2 = sub_tab.columns[2]

    # Add the table spanner to indicate which community setting
    sub_tab = GT(sub_tab).tab_spanner(
      label = md("**" + iv +"**"),
      columns = [col_1, col_2, "p-value"])

    display(sub_tab)
    print("\n") # Try printing a new line for .docx
    # cat("```{=openxml}", "```", sep = "\n")

```


# Correlation between each RE-AIM measure and each PRISM measure
```{python}
# Table 6 in the TableShell file
# Correlation between PRISM measures and RE-AIM measures including equity measures

reach = ["Reach", "Effectiveness", "Adoption", "Implementation Composite", 
'Implementation Adapt', 'Implementation Cost', 'Implementation Fidelity',
"Maintenance", "Reach Equity", "Effectiveness Equity", "Adoption Equity", 
"Maintenance Effectiveness Equity"]

prism = ["Expectation of Recipients", "Expectation of Organization", 
"Characteristics of Recipients", "Characteristics of Organization", 
"Implementation Sustainability Infrastructure", "External Environment"]

# Empty list to store results
correlation_results = []

for r_var in reach:
    for p_var in prism:
        # Compute Spearman correlation
        subset = tab1[[r_var, p_var]].dropna()
        r_value, p_value = spearmanr(subset[r_var], subset[p_var])
        
        # Modify the p-value
        p_value = format_pval_flt(p_value, 4)

        # Append results as a dictionary
        correlation_results.append({
            "Reach Variable": r_var,
            "PRISM Variable": p_var,
            "Spearman r-value": r_value.round(2),
            "p-value": p_value
        })


# GT(pd.DataFrame(correlation_results))
```

```{python}
# Create a correlation matrix--------------------------------------------------
cor_tab = tab1[reach + prism].corr(method = "spearman")

# Select only the prism columns
cor_tab = cor_tab[prism]

# Select only the reach rows
cor_tab = cor_tab.loc[reach]

# Display the correlation matrix
cor_tab.round(3)
```

```{python}
# eval: false
# A way to print the correlations as a GT table
# The rounding lookds good, but it looks better in .html because the bold
# headers are maintained
# GT(cor_tab.round(2).reset_index().rename(columns={'index': ''}))
```

```{python}
# Create a p value matrix
# Take the p-values and reshape them into a matrix
p_mat = pd.DataFrame(correlation_results)["p-value"].to_numpy().reshape(12, 6)

# Convert back to data frame
p_mat = pd.DataFrame(p_mat)

# Rename the index (rows)
p_mat.index = reach

# Rename the columns
p_mat.columns = prism

# Display the pvalue matrix
p_mat
```


```{python}
#| eval: false
cmap = sns.diverging_palette(230, 20, as_cmap=True)
plt.figure()
sns.heatmap(cor_tab, 
  annot=False, 
  cmap = cmap, 
  center = .5)
plt.show()
```


# Correlation between each RE-AIM measure and each equity RE-AIM measure
```{python}
# Table 7 in the TableShell file
# Correlation between RE-AIM and RE-AIM equity measures
reach = ["Reach", "Effectiveness", "Adoption", "Implementation Composite", 
"Maintenance"]

equity = ["Reach Equity", "Effectiveness Equity", "Adoption Equity", 
"Maintenance Effectiveness Equity"]


# Empty list to store results
eq_corr_results = []

for r_var in reach:
    for eq_var in equity:
        # Compute Spearman correlation
        subset = tab1[[r_var, eq_var]].dropna()
        r_value, p_value = spearmanr(subset[r_var], subset[eq_var])
        
        # Modify the p-value
        p_value = format_pval_flt(p_value, 4)

        # Append results as a dictionary
        eq_corr_results.append({
            "Reach Variable": r_var,
            "Equity Variable": eq_var,
            "Spearman r-value": r_value.round(2),
            "p-value": p_value
        })

# GT(pd.DataFrame(eq_corr_results))
```

```{python}
# Create a correlation matrix--------------------------------------------------
cor_tab = tab1[reach + equity].corr(method = "spearman")

# Select only the equity columns
cor_tab = cor_tab[equity]

# Select only the reach rows
cor_tab = cor_tab.loc[reach]

# Display the correlation matrix
cor_tab.round(3)
```

```{python}
# Create a p value matrix
# Take the p-values and reshape them into a matrix
p_mat = pd.DataFrame(eq_corr_results)["p-value"].to_numpy().reshape(5, 4)

# Convert back to data frame
p_mat = pd.DataFrame(p_mat)

# Rename the index (rows)
p_mat.index = reach

# Rename the columns
p_mat.columns = equity

# Display the pvalue matrix
p_mat
```


# Correlations between each RE-AIM measure
```{python}
# Correlations between RE-AIM measures and RE-AIM measures 
reach = ["Reach", "Effectiveness", "Adoption", "Implementation Composite",
'Implementation Adapt', 'Implementation Cost', 'Implementation Fidelity', 
"Maintenance"]

# Empty list to store results
reach_corr_results = []

for r_var in reach:
    for r_var2 in reach:
        # Compute Spearman correlation
        subset = tab1[[r_var, r_var2]].dropna()

        r_value, p_value = spearmanr(subset.iloc[:,0], subset.iloc[:,1])
        
        # Modify the p-value
        p_value = format_pval_flt(p_value, 4)

        # Append results as a dictionary
        reach_corr_results.append({
            "Reach Variable1": r_var,
            "Reach Variable2": r_var2,
            "Spearman r-value": r_value.round(2),
            "p-value": p_value
        })

# GT(pd.DataFrame(reach_corr_results))
```

```{python}
# Compute Spearman correlation matrix
reach_corr_matrix = tab1[reach].corr(method='spearman')
reach_corr_matrix.round(3)
```

```{python}
# Take the p-values and reshape them into a matrix
reach_p_matrix = pd.DataFrame(reach_corr_results)["p-value"].to_numpy().reshape(8, 8)

# Fill the diagonal with 0s
np.fill_diagonal(reach_p_matrix, " ")

# Convert back to data frame
reach_p_matrix = pd.DataFrame(reach_p_matrix)

# Rename the index
reach_p_matrix.index = reach

# Rename the columns
reach_p_matrix.columns = reach

# Display the pvalue matrix
reach_p_matrix
```


# Correlations between each Equity measure
```{python}
# Correlations between Equity measure
equity = ['Reach Equity', 'Effectiveness Equity', 'Adoption Equity', 'Maintenance Effectiveness Equity']

# Empty list to store results
equity_corr_results = []

for r_var in equity:
    for r_var2 in equity:
        # Compute Spearman correlation
        # Compute Spearman correlation
        subset = tab1[[r_var, r_var2]].dropna()

        r_value, p_value = spearmanr(subset.iloc[:,0], subset.iloc[:,1])
        # r_value, p_value = spearmanr(tab1[r_var], tab1[r_var2])
        
        # Modify the p-value
        p_value = format_pval_flt(p_value, 4)

        # Append results as a dictionary
        equity_corr_results.append({
            "Equity Variable1": r_var,
            "Equity Variable2": r_var2,
            "Spearman r-value": r_value.round(2),
            "p-value": p_value
        })

# GT(pd.DataFrame(reach_corr_results))
```

```{python}
# Compute Spearman correlation matrix
equity_corr_matrix = tab1[equity].corr(method='spearman')
equity_corr_matrix.round(3)
```

```{python}
# Take the p-values and reshape them into a matrix
equity_p_matrix = pd.DataFrame(equity_corr_results)["p-value"].to_numpy().reshape(4, 4)

# Fill the diagonal with 0s
np.fill_diagonal(equity_p_matrix, " ")

# Convert back to data frame
equity_p_matrix = pd.DataFrame(equity_p_matrix)

# Rename the index
equity_p_matrix.index = equity

# Rename the columns
equity_p_matrix.columns = equity

# Display the pvalue matrix
equity_p_matrix
```


# Correlations between each PRISM measure
```{python}
# Correlations between PRISM measure

# Empty list to store results
prism_corr_results = []

for p_var in prism:
    for p_var2 in prism:
        # Compute Spearman correlation
        # Compute Spearman correlation
        subset = tab1[[r_var, r_var2]].dropna()

        r_value, p_value = spearmanr(subset.iloc[:,0], subset.iloc[:,1])
        # r_value, p_value = spearmanr(tab1[p_var], tab1[p_var2])
        
        # Modify the p-value
        p_value = format_pval_flt(p_value, 4)

        # Append results as a dictionary
        prism_corr_results.append({
            "Reach Variable1": p_var,
            "Reach Variable2": p_var2,
            "Spearman r-value": r_value.round(2),
            "p-value": p_value
        })

# GT(pd.DataFrame(prism_corr_results))
```

```{python}
# Compute Spearman correlation matrix
prism_corr_matrix = tab1[prism].corr(method='spearman')
prism_corr_matrix.round(3)
```

```{python}
# Take the p-values and reshape them into a matrix
prism_p_matrix = pd.DataFrame(prism_corr_results)["p-value"].to_numpy().reshape(6, 6)

# Fill the diagonal with 0s
np.fill_diagonal(prism_p_matrix, " ")

# Convert back to data frame
prism_p_matrix = pd.DataFrame(prism_p_matrix)

# Rename the index
prism_p_matrix.index = prism

# Rename the columns
prism_p_matrix.columns = prism

# Display the pvalue matrix
prism_p_matrix
```


# Intraclass correlation coefficients for each measure
- Intraclass correlation coefficients calculated for those with two or more entries per team.
- Data available for 22 teams
- Data available for 127 participants
```{python}
# Intraclass correlation within team members
icc_data = df.copy()

# Drop missing values in teamcode
icc_data = icc_data.dropna(subset=['teamcode'])

# Convert all values to lower case
icc_data['teamcode'] = icc_data['teamcode'].str.lower()

# strip away blanks
icc_data['teamcode'] = icc_data['teamcode'].str.replace(" ", "")

# Check values here

# then find out who has more than one value per group
teams = icc_data.groupby('teamcode').size().reset_index(name = "count").query("count > 1")

# Number of rows
teams['count'].sum()

# filter the icc data
icc_data = icc_data[icc_data['teamcode'].isin(teams['teamcode'])]

# Create a composite of Iadap Icost Ifid
icc_data["Icomp"] = icc_data[["Iadapt", "Icost", "Ifid"]].mean(axis=1)

# Rename columns
icc_data = icc_data.rename(columns=col_name_dict)

# Write out the icc_data to csv
icc_data.to_csv(proj_root + "\\data\\icc_data.csv")

# Create a test df consisting of just the reach variable amd teamcode for the 
# development of icc +/- 95% ci calculations in Python and R to check against
# results produced with the SAS approach in this script.
test_df = icc_data[["Reach", "teamcode"]].copy()
test_df = test_df.dropna()

# Save the test_df to .csv to double check with R or SAS
test_df.to_csv("C:\\Users\\rodrica2\\OneDrive - The University of Colorado Denver\\Documents\\DFM\\projects\\iPRISM\\data\\icc_test.csv")
```

```{python}
#| echo: false
# Set up a sas session
sas = saspy.SASsession(cfgname = 'autogen_winlocal', verbose = False);

# Prepare an empty list to store results
icc_results = pd.DataFrame()

# Combine all of the prism/re-aim variables
all_variables = prism + reach + equity

# Loop through each measure column
for measure in all_variables:  # Add more measures as needed
    # Ensure that there are two or more entries per measure
    temp = icc_data[['teamcode', measure]].copy()

    #
    teams = temp.groupby('teamcode').size().reset_index(name = "count").query("count > 1")

    # Number of rows
    teams['count'].sum()

    # filter the icc data
    temp = temp[temp['teamcode'].isin(teams['teamcode'])]

    # Create a data frame where only the teamcode and the measure are present
    df_to_sas = temp.copy()
    df_to_sas = df_to_sas[['teamcode', measure]].reset_index(drop = True)

    # rename the Prism/Re-AIM measure to "measure" to assist in looping
    df_to_sas["measure"] = df_to_sas[measure]

    # Drop the column otherwise the length of the name is too long for SAS
    df_to_sas.drop(columns=[measure], inplace=True)

    # SAS section begin -------------------------------------------------------
    # Send the sas_df to SAS
    sas_data = sas.df2sd(df_to_sas, verbose = False)

    # Submit SAS commands, use sas.submitLST() to display output in viewer
    c = sas.submit("""
    *ods select none;
    ods output CovParms=covp;

    proc mixed data = work._df covtest;
      class teamcode;
      model measure = /s;
      random intercept/subject = teamcode;
    run;

    /* Jun's approach -------------------------------------------------------*/
    data icc_jun;
      set covp;
      retain bvar var_bvar;
      
      if CovParm = "Intercept" then do;
        bvar = Estimate; /* Between-group variance */
        var_bvar = StdErr**2; /* Variance of between-group variance */
      end;
      
      if CovParm = "Residual" then do;
        wvar = Estimate; /* Within-group variance */
        var_wvar = StdErr**2; /* Variance of within-group variance */

        /* Compute ICC */
        icc = bvar / (bvar + wvar);

        /* Compute Variance of ICC using Jun's method */
        var_icc = var_bvar * ((wvar**2) / (wvar + bvar)**4) + var_wvar * ((bvar**2) / (wvar + bvar)**4);
        se_icc = sqrt(var_icc);  /* Standard error of ICC */

        /* Compute 95% Confidence Interval */
        icc_LCL = icc - (1.96 * se_icc); /* Lower bound */
        icc_UCL = icc + (1.96 * se_icc); /* Upper bound */

        output;
      end;
    run;

    proc print data=icc_jun;
      var icc var_icc se_icc icc_LCL icc_UCL;
      title "ICC, Variance, SE, and 95% Confidence Interval (Delta Method)";
    run;

    """)

    sas_to_pd = sas.sasdata("icc_jun", libref = "work")
    
    # Convert the SAS dataset to a Pandas DataFrame
    sas_icc = sas_to_pd.to_df()

    # Calculate the Z value and the p value
    sas_icc["z-value"] = sas_icc["icc"]/sas_icc["se_icc"]

    # Compute two-tailed p-value
    pval = 2 * (1 - norm.cdf(abs(sas_icc["z-value"])))

    # the two_tailed test returns an array, convert to float
    pval = pval[0]

    # Format the p-value to 4 digits
    pval = format_pval_flt(pval, 4)

    sas_icc["p-value"] = pval

    # Format the ICCs
    sas_icc['icc_LCL'] = sas_icc['icc_LCL'].round(2)
    sas_icc['icc_UCL'] = sas_icc['icc_UCL'].round(2)

    # Set the LCL to zero if it's negative
    sas_icc["icc_LCL"] = sas_icc["icc_LCL"].clip(lower=0)
    
    # Format the ICCs
    sas_icc['icc'] = sas_icc['icc'].round(2)

    # Select the row and columns of interest 
    sas_icc = sas_icc[["icc", "p-value", "icc_LCL", "icc_UCL"]]

    # Reset the index to the variable measure
    sas_icc.index = [measure]    

    icc_results = pd.concat([icc_results, sas_icc], axis=0)

```

```{python}
# Display the ICC results
icc_results
```


# Comparisons between stages
```{python}
# Create Icomp1, Icomp2, Icomp3 to refer to the planning, implementing, sustaining phases
df['Icomp1'] = df[['Iadapt1', 'Icost1', 'Ifid1']].mean(axis = 1, skipna = True)

df['Icomp2'] = df[['Iadapt2', 'Icost2', 'Ifid2']].mean(axis = 1, skipna = True)

df['Icomp3'] = df[['Iadapt3', 'Icost3', 'Ifid3']].mean(axis = 1, skipna = True)

# Add the Icomp to the prefixes
prefixes = prefixes + ['Icomp']
```

```{python}
#| eval: false
n_df = pd.DataFrame()

for prefix in prefixes:
    temp = df[[prefix + '1', prefix + '2', prefix + '3']].copy()
    ns = pd.DataFrame(temp.count()).T
    temp['non_missing_count'] = temp.count(axis=1)

    n_df = pd.concat([n_df, ns], axis = 1 )

# temp.count()
```

```{python}
# Create an empty data frame to hold results
pairwise_results = pd.DataFrame()

for prefix in prefixes:
    # print(prefix)
    temp = df[[prefix + '1', prefix + '2', prefix + '3']].copy()
    
    # Create a table of sample sizes
    ns = pd.DataFrame(temp.count()).T

    # Create a table of means
    means = pd.DataFrame(temp.mean(axis=0, skipna=True)).round(2).T

    # Create a table of SEs
    sems = pd.DataFrame(temp.sem(axis = 0, skipna=True)).round(2).T
    
    # Create a data frame with means and SEMs in the same cell
    combined_df = ns.astype(str) + "; " + means.astype(str) + " (" + sems.astype(str) + ")"

    # Rename the columns
    combined_df = combined_df.rename(columns={prefix + '1': f"(1) Planning",
                          prefix + '2': f"(2) Implementing",
                          prefix + '3': f"(3) Sustaining"})



    # Comparison 1
    tstat1, pval1 = ttest_ind(temp[[prefix + '1']], temp[[prefix + '2']], nan_policy='omit', equal_var=False)

    # Modify the p-value
    pval1 = format_pval_flt(pval1[0], 4)

    # Comparison 2
    tstat2, pval2 = ttest_ind(temp[[prefix + '1']], temp[[prefix + '3']], nan_policy='omit', equal_var=False)

    # Modify the p-value
    pval2 = format_pval_flt(pval2[0], 4)

    # Comparison 3
    tstat3, pval3 = ttest_ind(temp[[prefix + '2']], temp[[prefix + '3']], nan_policy='omit', equal_var=False)

    # Modify the p-value
    pval3 = format_pval_flt(pval3[0], 4)


    # Store results in a data frame
    results = pd.DataFrame([{"(1) vs (2)": pval1,
    "(1) vs (3)": pval2,
    "(2) vs (3)": pval3}])

    # join the pairwise comparisons
    pairwise = pd.concat([combined_df, results], axis = 1)
    pairwise.index = [prefix]

    pairwise_results = pd.concat([pairwise_results, pairwise], axis = 0)


```

```{python}
# reorder the indexes
pairwise_results = pairwise_results.loc[order]
```

```{python}
# Reorder the dictionary according to the list order
# ordered_dict = {key: col_name_dict[key] for key in order}

# Rename the indexes
pairwise_results = pairwise_results.rename(index=col_name_dict)
```

```{python}
# Display the table, add tab _spanner
# pairwise_results
```

```{python}
# The names of the columns in pairwise_results to be able to index them when
# setting the tab spanner
col_names = pairwise_results.columns

GT(pairwise_results.reset_index().rename(columns={'index': ''})).tab_spanner(
      label = md("**N; Mean (SE)**"),
      columns = [col_names[0], col_names[1], col_names[2]]).tab_spanner(
        label= md("**P-value**"),
        columns = ["(1) vs (2)", "(1) vs (3)", "(2) vs (3)"]
      )

```


# LMER Model PRISM ISI vs All Others
<!-- LINEAR MIXED MODEL WITH PRISM -->

```{python}
# Need tab1 and need the participant identifiers
h1 = tab3[prism].copy()

# Get the participant identifier as a column
h1 = h1.reset_index().rename(columns={"User Id": "id"})

h1 = h1.melt(
    id_vars="id",  # column(s) to keep fixed
    var_name="domain",         # name of the new 'variable' column
    value_name="Y"             # name of the new 'value' column
)

# Recode the variables before passing to SAS
h1["domain"] = h1["domain"].replace({prism[0]: "ExpRec", 
                                     prism[1]: "ExpOrg",
                                     prism[2]: "ChrRec", 
                                     prism[3]: "ChrOrg",
                                     prism[4]: "ISI", 
                                     prism[5]: "ExtEnv"})

```

```{python}
# sas = saspy.SASsession(cfgname = 'autogen_winlocal', verbose = False);

# Send the sas_df to SAS
sas_data = sas.df2sd(h1, verbose = False)

  # Submit SAS commands, use sas.submitLST() to display output in viewer
c = sas.submit("""
ods output 
  Tests3=type3_results 
  Estimates=estimates_table 
  LSMeans=lsmeans_table 
  Diffs=lsmeans_diffs;

proc mixed data = work._df;
  class id domain;
  model Y = domain;
  random intercept/subject = id;

  /* Request all pairwise comparisons with Tukey adjustment */
  lsmeans domain / pdiff=all;

  /* ESTIMATE: ISI vs average of others*/
  estimate "ISI vs Others" domain -0.2 -0.2 -0.2 -0.2 -0.2 1;

run;

""")
```

## Type 3 Tests of Fixed Effects
```{python}
type3_results = sas.sasdata("type3_results", libref = "work").to_df()
type3_results["ProbF"] = format_pval_df(type3_results['ProbF'])
type3_results.round(2).set_index("Effect")
```

## Estimates
```{python}
estimates = sas.sasdata("estimates_table", libref = "work").to_df()
estimates["Probt"] = format_pval_df(estimates["Probt"])
estimates.round(2).set_index("Label")
```

## LS Means
```{python}
lsmeans = sas.sasdata("lsmeans_table", libref = "work").to_df()
lsmeans["Probt"] = format_pval_df(lsmeans["Probt"])
lsmeans.round(2).set_index("Effect")
```

## Differences of Least Squares Means
```{python}
diffs = sas.sasdata("lsmeans_diffs", libref = "work").to_df()
diffs["Probt"] = format_pval_df(diffs["Probt"])
diffs.round(2).set_index("Effect")
```


# LMER Model REAIM
```{python}

# Need tab1 and need the participant identifiers
h1 = tab3[reach].copy()

# Get the participant identifier as a column
h1 = h1.reset_index().rename(columns={"User Id": "id"})

h1 = h1.melt(
    id_vars="id",  # column(s) to keep fixed
    var_name="domain",         # name of the new 'variable' column
    value_name="Y"             # name of the new 'value' column
)
```

```{python}
# sas = saspy.SASsession(cfgname = 'autogen_winlocal', verbose = False);

# Send the sas_df to SAS
sas_data = sas.df2sd(h1, verbose = False)

  # Submit SAS commands, use sas.submitLST() to display output in viewer
c = sas.submit("""
ods output 
  Tests3=type3_results 
  Estimates=estimates_table 
  LSMeans=lsmeans_table 
  Diffs=lsmeans_diffs;

proc mixed data = work._df;
  class id domain;
  model Y = domain;
  random intercept/subject = id;

  /* Request all pairwise comparisons with Tukey adjustment */
  lsmeans domain / pdiff=all;

run;

""")
```

## Type 3 Tests of Fixed Effects
```{python}
type3_results = sas.sasdata("type3_results", libref = "work").to_df()
type3_results["ProbF"] = format_pval_df(type3_results['ProbF'])
type3_results.round(2).set_index("Effect")
```

## LS Means
```{python}
lsmeans = sas.sasdata("lsmeans_table", libref = "work").to_df()
lsmeans["Probt"] = format_pval_df(lsmeans["Probt"])
lsmeans.round(2).set_index("Effect")
```

## Differences of Least Squares Means
```{python}
diffs = sas.sasdata("lsmeans_diffs", libref = "work").to_df()
diffs["Probt"] = format_pval_df(diffs["Probt"])
diffs.round(2).set_index("Effect")
```

# LMER Model REAIM & PRISM
```{python}

# Need tab1 and need the participant identifiers
h1 = tab3[reach + prism].copy()

# Get the participant identifier as a column
h1 = h1.reset_index().rename(columns={"User Id": "id"})

h1 = h1.melt(
    id_vars="id",  # column(s) to keep fixed
    var_name="domain",         # name of the new 'variable' column
    value_name="Y"             # name of the new 'value' column
)

# Recode the variables before passing to SAS
h1["domain"] = h1["domain"].replace({prism[0]: "ExpRec", 
                                     prism[1]: "ExpOrg",
                                     prism[2]: "ChrRec", 
                                     prism[3]: "ChrOrg",
                                     prism[4]: "ISI", 
                                     prism[5]: "ExtEnv"})
```

```{python}
# sas = saspy.SASsession(cfgname = 'autogen_winlocal', verbose = False);

# Send the sas_df to SAS
sas_data = sas.df2sd(h1, verbose = False)

  # Submit SAS commands, use sas.submitLST() to display output in viewer
c = sas.submit("""
ods output 
  Tests3=type3_results 
  Estimates=estimates_table 
  LSMeans=lsmeans_table 
  Diffs=lsmeans_diffs;

proc mixed data = work._df;
  class id domain;
  model Y = domain;
  random intercept/subject = id;

  /* Request all pairwise comparisons with Tukey adjustment */
  lsmeans domain / pdiff=all;

run;

""")
```

## Type 3 Tests of Fixed Effects
```{python}
type3_results = sas.sasdata("type3_results", libref = "work").to_df()
type3_results["ProbF"] = format_pval_df(type3_results['ProbF'])
type3_results.round(2).set_index("Effect")
```

## LS Means
```{python}
lsmeans = sas.sasdata("lsmeans_table", libref = "work").to_df()
lsmeans["Probt"] = format_pval_df(lsmeans["Probt"])
lsmeans.round(2).set_index("Effect")
```

## Differences of Least Squares Means
```{python}
pd.set_option('display.max_rows', None)

diffs = sas.sasdata("lsmeans_diffs", libref = "work").to_df()
diffs["Probt"] = format_pval_df(diffs["Probt"])
diffs.round(2).set_index("Effect")
```

# REAIM item vs corresponding equity versions
```{python}
# T1 Reach vs Equity versions
reach_non_eq = ["Reach", "Effectiveness", "Adoption", "Maintenance"]

# Prepare an empty list to store results
reach_vs_eq = []


for i in range(len(reach_non_eq)):
    # print(i)

    # Get the values for ith position in each vector
    var_1 = reach_non_eq[i]

    var_2 = equity[i]

    temp = tab3[[var_1, var_2]].copy()

    # Get the participant identifier as a column
    temp = temp.reset_index().rename(columns={"User Id": "id"})

    # Convert to long
    temp = temp.melt(
        id_vars="id",  # column(s) to keep fixed
        var_name="domain",         # name of the new 'variable' column
        value_name="Y"             # name of the new 'value' column
    )

    # Send to SAS
    sas_data = sas.df2sd(temp, verbose = False)

    # Submit SAS commands, use sas.submitLST() to display output in viewer
    c = sas.submit("""
    ods output 
      Tests3=type3_results 
      LSMeans=lsmeans_table 
      Diffs=lsmeans_diffs;

    proc mixed data = work._df;
      class id domain;
      model Y = domain;
      random intercept/subject = id;

      /* Request all pairwise comparisons with Tukey adjustment */
      lsmeans domain / pdiff=all;

    run;

    """)

    # Collect and format all of the tables from SAS
    # Type 3 results
    type3_results = sas.sasdata("type3_results", libref = "work").to_df()
    type3_results["ProbF"] = format_pval_df(type3_results['ProbF'])
    # type3_results.round(2).set_index("Effect")

    # LS Means
    lsmeans = sas.sasdata("lsmeans_table", libref = "work").to_df()
    lsmeans["Probt"] = format_pval_df(lsmeans["Probt"])
    # lsmeans.round(2).set_index("Effect")


    # Diffs
    diffs = sas.sasdata("lsmeans_diffs", libref = "work").to_df()
    diffs["Probt"] = format_pval_df(diffs["Probt"])
    # diffs.round(2).set_index("Effect")

    # Collect the individual results into an inner list
    tab_set = [type3_results, lsmeans, diffs]

    # Store it in the outer list
    reach_vs_eq.append(tab_set)


```

```{python}
# For each element in the reach_vs_eq results loop through the inner list to round and set the index
reach_vs_eq = [[df.round(2).set_index("Effect") for df in inner_list] for inner_list in reach_vs_eq]

# In the remaining output:
# Each domain is indexed by [x][y], where x is a value of 0-3
#  where each x represents a domain Reach, effectiveness, adoption, maintenance
#  and y is a value between 0-2 and represents separate tables such as the
#  the type3 results, the ls means, and the differences in means
```

## Reach vs Reach Equity
### Type3
```{python}
reach_vs_eq[0][0]
```

### LS Means
```{python}
reach_vs_eq[0][1]
```

### Diffs
```{python}
display(reach_vs_eq[0][2])
```

## Effectiveness vs Effectiveness Equity
### Type3
```{python}
reach_vs_eq[1][0]
```

### LS Means
```{python}
reach_vs_eq[1][1]
```

### Diffs
```{python}
reach_vs_eq[1][2]
```

## Adoption vs Adoption Equity
### Type3
```{python}
reach_vs_eq[2][0]
```

### LS Means
```{python}
reach_vs_eq[2][1]
```

### Diffs
```{python}
reach_vs_eq[2][2]
```

## Maintenance vs Maintenance Equity
### Type3
```{python}
reach_vs_eq[3][0]
```

### LS Means
```{python}
reach_vs_eq[3][1]
```

### Diffs
```{python}
reach_vs_eq[3][2]
```
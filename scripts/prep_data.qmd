---
title: "iPRISM Web Tool Data Overview"
# format: docx # if output is docx, table formatting from SAS output is lost
format:
   html:
     embed-resources: true
     toc: true

#format: html

execute: 
  echo: false
  warning: false
jupyter: iprism
---

```{python}
# MAIN TABLES TO PRODUCE 
# From TableShell jy021325 - Copy_BF

# 1 3b 6 7 8 10
```

```{python, import libraries}
# Data manipulation tools
import os
import pandas as pd
import numpy as np

# Tables and plotting
from great_tables import GT, md, html
import matplotlib.pyplot as plt
import seaborn as sns

# Tables & statistics
import saspy
from scipy.stats import ttest_ind


# Custom functions
# iprism_fx found in Lib/site-packages
from iprism_fx import tables
```

```{python, configure sas}
# Start a SAS session to connect python and SAS
# Use verbose = False to prevent printing the session ID to console
sas = saspy.SASsession(cfgname = 'autogen_winlocal', verbose = False);
```

```{python, read data}
# Specify the project root directory
proj_root = 'C:\\Users\\rodrica2\\OneDrive - The University of Colorado Denver\\Documents\\DFM\\projects\\iPRISM'

# Specify the file to import
file = proj_root + "\\data\\iPRISM_entry_data_11.5.24.csv"
# file = proj_root + "\\data\\iPRISM_entry_data_1.30.25.csv"

# Import data
df = pd.read_csv(file, header = 0)
```

```{python}
# COALESCE THE TWO HEADERS, THEN SET ROW 1 AS LABELS AND ROW 2 AS COLUMN NAMES
# Get the 1st row of headers as they appear in the .csv file. These will become
# the column labels for displaying
column_labels = pd.DataFrame(df.columns.tolist(), columns = ['labels'])

# Get the 2nd row of headers since they contain shorthand names for the columns
column_names = pd.DataFrame(df.iloc[0].tolist(), columns = ['names'])

# Create a new data frame where column names and labels are coalesced
new_column_names = pd.concat([column_names, column_labels], axis=1)

# Create a new column of the coalesced names and labels
new_column_names['coalesced'] = new_column_names.names.combine_first(new_column_names.labels)

df.columns = new_column_names['coalesced']

# Remove the first row since it contains the headers as column names
df.drop(index=df.index[0], axis=0, inplace=True)

# Rename the axis column to none
df = df.rename_axis(None, axis=1)
```

```{python}
# IDENTIFY "TEST" RECORDS VIA THE PROGNAME COLUMN -----------------------------
# Remove records where the value contains "test" in the progname column
# First fill in the missing values with a missing value to use str.contains()
# otherwise str.contains() will fail on NAs
df['progname'] = df['progname'].fillna("missing")

before_n = len(df)
# print("Number of rows before dropping 'test' records:", len(df))
# df starts out as 490 rows, but 40 rows are dropped for containing "test"
# in progname. CR 12/06/2024

# Create a column where progname is all lower case to later on remove any row
# where progname == 'test'
df['progname_lower'] = df['progname'].str.lower()

# Index the rows that do not have "test" in the all lower cases progname
df = df[~df['progname_lower'].str.contains("test")]

after_n = len(df)
# print("Number of rows after dropping 'test' records:", len(df), 'a difference of ', before_n - after_n, ' rows.')

# Reset the missing to NaN
df.loc[df['progname'] == "missing", 'progname'] = np.nan
```

```{python}
# DETERMINE OTHER WAYS OF IDENTIFYING TEST RECORDS OR RECORDS WHERE THE 
# RESPONSES ARE NOT SERIOUS
```

```{python}
#progname
# Replace dubious program names
values_to_flag = ['Cindy is the best',
'Cool cats',
'Cool stuff',
'Destroy the one ring',
'Fun times',
"Russ's fantasy",
'a',
'aa',
'asfd',
'cool stuff',
'stuff',
'ty',
'wefq',
'xyz']

# Replace any value in progname that contains the values to replace with None
# df['progname'] = df['progname'].replace(values_to_replace, None)

df['flag'] = df['progname'].isin(values_to_flag).astype(int)
```

```{python}
# LEFT OFF HERE  ##############################################################
# NEED TO SEND A LIST OF DUBIOUS RECORDS TO BRYAN
test = df[df['flag'] == 1]
# test.to_csv(proj_root + "\\data\\records_to_review.csv")

# Sent to BRYAN but have not heard back as of 2/6/2024
```

```{python}
# Collapse PASOS program values
values_to_replace = ["PASOS", "PASOS Program", "Pasos Program"]
df['progname'] = df['progname'].replace(values_to_replace, "PASOS Program")

# Collapse Tobacco cessation program
df['progname'] = df['progname'].replace("Tobacco cessation program", "Tobacco Cessation Program")
```

```{python}
# Replace variations of PTJ in the teamcode column
df['teamcode'] = (df['teamcode']
  .str.replace('ptj', 'PTJ')
  .str.replace('Ptl', 'PTJ')
  .str.replace('PTJ ', 'PTJ')
)
```

```{python}
# jobtitle
values_to_replace = ["Specialist - TEST!",
"a",
"test",
"Bryan",
"Gandalf the Grey",
"Jane Doe"]

df['jobtitle'] = df['jobtitle'].replace(values_to_replace, None)

# Values to collapse
df['jobtitle'] = df['jobtitle'].replace("Intensivista pediatra", "Intensivista Pediatra")


values_to_replace = ["Assistant Professor of Neurology",
"Assistant professor"]

df['jobtitle'] = df['jobtitle'].replace(values_to_replace, "Assistant Professor")


df['jobtitle'] = df['jobtitle'].replace("Assoc Prof|assoc prof", "Associate Profesor")


df['jobtitle'] = df['jobtitle'].replace("Physican", "Physician")


df['jobtitle'] = df['jobtitle'].replace("Principal investigator", "Principal Investigator")


df['jobtitle'] = df['jobtitle'].replace("reasearch", "research")
```

<!-- # Likert Variables -->
```{python}
#Aper1:Aper3 n.b. these are out of order it's: 1, 3, 2
#Aeq1:Aeq3
#Astper1:Astper3
#Astint1:Astint3
#Astdec1:Astdec3
#Ifid1:Ifid3
#ladapt1:ladapt3
#lcost1:lcost3
#Rper1:Rper3
#Req1:Req3
#E1:E3
#Eeq1:Eeq3
#Mper1:Mper3
#Madapt1:Madapt3
#Meff1:Meff3
#Meffeq1:Meffeq3
#EXPcom1:EXPcom3
#exorg1:EXPorg3 n.b. If exorg1 is part of this,then rename the column to maintain pattern recognition
#CHcom1:CHcom3
#CHorg1:CHorg3
#res1:res3
#CHexenv1:CHexenv3

# Rename exorg1 as EXPorg1. Confirmed by Bryan 12-16-2024
df['EXPorg1'] = df['exorg1']

prefixes = [
'Aper',
'Aeq',
'Astper',
'Astint',
'Astdec',
'Ifid',
'Iadapt',
'Icost',
'Rper',
'Req',
'E',
'Eeq',
'Mper',
'Madapt',
'Meff',
'Meffeq',
'EXPcom',
'EXPorg',
'CHcom',
'CHorg',
'res',
'CHexenv']

for prefix in prefixes:
  # print(prefix)
  temp = df[[prefix + '1', prefix + '2', prefix + '3']].copy()
  col_name = prefix
  temp[col_name] = pd.to_numeric(temp.bfill(axis=1).iloc[:,0])
  df = pd.concat([df, temp[col_name]], axis = 1)
```

```{python}
# Create a composite variable
# Rename the columns
```

<!-- ******************************** FENCE BEGIN ************************* -->
## Table 1
```{python}
# Create a table of the prism and re-aim values only
tab1 = df[prefixes].copy()
```

```{python}
# Create a composite of Iadap Icost Ifid
tab1["Icomp"] = tab1[["Iadapt", "Icost", "Ifid"]].mean(axis=1)
```

```{python}
# Reorder the columns
order = [
  "EXPcom",
  "EXPorg",
  "CHcom",
  "CHorg",
  "res",
  "CHexenv",
  "Rper",
  "E",
  "Aper",
  "Icomp",
  "Iadapt",
  "Icost",
  "Ifid",
  "Mper",
  "Req",
  "Eeq",
  "Aeq",
  "Meffeq"
]

# Reorder the tab1 data frame
tab1 = tab1[order]
```

```{python}
# Rename columns to display in a table
tab1 = tab1.rename(columns={
  "Aper": "Adoption", 
  "Aeq": "Adoption Equity",
  'Astper': "Adoption Staff",
  'Astint': "Adoption Intended",
  'Astdec': "Adoption Declined",
  'Ifid': "Implementation Fidelity",
  'Iadapt': "Implementation Adapt",
  'Icost': "Implementation Cost",
  'Icomp': "Implementation Composite",
  'Rper': "Reach",
  'Req': "Reach Equity",
  'E': "Effectiveness",
  'Eeq': "Effectiveness Equity",
  'Mper': "Maintenance",
  'Madapt': "Maintenance Adaptation",
  'Meff': "Maintenance Effectiveness",
  'Meffeq': "Maintenance Effectiveness Equity",
  'EXPcom': "Expectation of Recipients",
  'EXPorg': "Expectation of Organization",
  'CHcom': "Characteristics of Recipients",
  'CHorg': "Characteristics of Organization",
  'res': "Implementation Sustainability Infrastructure",
  'CHexenv': "External Environment"
  })
```


```{python}
# Create a summary dataframe
summary_tab1 = pd.DataFrame({
    "N": tab1.count(),
    "Mean": tab1.mean().round(2),
    "StdDev": tab1.std().round(2),
    "Median": tab1.median(),
    "Min": tab1.min(),
    "Max": tab1.max()
}).reset_index()

# Rename the index column to something descriptive
summary_tab1.rename(columns={"index": "Measure"}, inplace=True)

# Display the table
GT(summary_tab1)
```

## Table 2
```{python}
# Table 2 refers to table 3b in the TableShell file
# Mean +- SE for each variable and P-value

# Create a new data frame
tab2 = df[["Community", "Clinical", "Public health", "Other.1"]].copy()  #.mean(axis=1)

# Rename the Other.1 Column
tab2.rename(columns={"Other.1": "Other"}, inplace=True)

# Convert all values to binary
tab2 = tab2.notna().astype(int)

# Get a row sum to make a new variable called multiple
tab2["Multiple"] = tab2.sum(axis = 1)

tab2["Multiple"] = np.where(tab2["Multiple"] > 0, 1, tab2["Multiple"])

# Merge the two data frames to conduct t-tests
tab2 = pd.concat([tab2, tab1], axis = 1)
```

```{python}
def format_p_value(p):
    if p < 0.001:
        return "<0.001"
    else:
        return f"{p:.2f}"

```

```{python}
# Run the t-tests
results = []
dependent_vars = tab1.columns.tolist()
independent_vars = ["Community", "Clinical", "Public health", "Other", "Multiple"]

for iv in independent_vars:
    for dv in dependent_vars:
        # Check unique groups in the IV
        groups = tab2[iv].unique()
        if len(groups) != 2:
            raise ValueError(f"Expected exactly two groups in {iv}, found {len(groups)}")

        # Split data into the two groups based on IV
        group_1 = tab2.loc[tab2[iv] == groups[1], dv]
        group_2 = tab2.loc[tab2[iv] == groups[0], dv]

        # Compute means and standard errors
        group_1_mean = group_1.mean().round(2)
        group_2_mean = group_2.mean().round(2)
        group_1_se = group_1.sem().round(2)
        group_2_se = group_2.sem().round(2)

        # Perform t-test
        t_stat, p_val = ttest_ind(group_1, group_2, equal_var=False)

        # Modify the p-value
        p_val = format_p_value(p_val)

      
        # Store the result
        results.append({
          "Dependent Variable": [dv],
          "Independent Variable": [iv],
          "Yes Mean +/- SE": [f"{group_1_mean} +/- {group_1_se}"],
          "No Mean +/- SE": [f"{group_2_mean} +/- {group_2_se}"],
          "p-value": [p_val]
          })
        
        # Add a spanning header
        # results = results.tab_spanner(label = iv, columns=["Yes Mean +/- SE", "No Mean +/- SE", "p-value"])
    
    # Convert results for this IV into a DataFrame
    # results_by_iv[iv] = pd.DataFrame(results)

```

```{python}
# Display Table 2
GT(pd.DataFrame(results))
```

## Table 3
```{python}
# Table 6 in the TableShell file
# Correlation between PRISM measures and RE-AIM measures including equity measures
```

## Table 4
```{python}
# Table 7 in the TableShell file
# Correlation between RE-AIM and RE-AIM equity measures
```

## Table 5
```{python}
# Correlations between RE-AIM measures and RE-AIM measures 
```

<!-- ******************************** FENCE END *************************** -->

<!-- EVERYTHING BELOW HERE MAY NOT BE NECESSARY -->
<!-- ## Indvidual vs Team -->
```{python}
# tables.freq_prop(df, 'indvteam')
```

<!-- ## New vs existing Team -->
<!-- - team_nx is branched for those that selected 'team' in indvteam -->
```{python}
# tables.freq_prop(df[df['indvteam'] == 'team'], 'team_nx' )
```

<!-- ## Implementation stage -->
```{python}
# tables.freq_prop(df, 'impstage')
```

```{python}
#| warning: false
#| eval: false

# sas.get_member_names(libref='work')
data_team_job = df[['indvteam', 'team_nx', 'impstage']]

sas_data = sas.df2sd(data_team_job, verbose = False)

# cols = pd.DataFrame(df.drop(columns=prefixes).columns)

c = sas.submitLST("""
proc freq data = work._df;
  tables indvteam;
run;

proc freq data = work._df;
  where indvteam = 'team';
  tables team_nx;
run;

proc freq data = work._df;
  tables impstage;
run;

""")

# Is team_nx branched to only those with team
```

<!-- ## Jobrole -->
<!-- - Jobrole is a select all that apply variable, proportions will not add up to 100%. -->
```{python}
# jobroles = ['Clinician', 'Quality improvement specialist', 'Researcher', 'Public health practitioner', 'Implementation specialist', 'Clinical administrator (e.g., Medical director)', 'Academic administrator (e.g., Department chair, dean)', 'Other']
```

```{python}
#| eval: false
# tables.all_apply(df, jobroles, 'Characteristic', True)
```

```{python}
# eval: false
# test = df[jobroles]
```

```{python}
# # Placed here to see what SAS does to a data set with check all that apply. Looks like it needs to be recoded
# #| warning: false
# #| eval: false

# # Create a separate dataset with the columns of interest only, then rename the column names
# # to meet SAS criteria of 32 bytes
# data_jobroles = df[jobroles]
# # data_jobroles = data_jobroles.notna().astype(int)

# # Rename columns to a shorter name
# max_length = 10
# data_jobroles.columns = [col[:max_length] for col in data_jobroles.columns]

# # Convert pd to sas
# sas_data = sas.df2sd(data_jobroles)
```

```{python}
#| eval: false
c = sas.submitLST("""
proc freq data = work._df;
  table _character_ / missing;
run;
""")
```

<!-- ### Those that selected other for jobrole (free text) -->
```{python}
#| eval: false
data_other = df[df['Other'] == 'Other']

data_other.columns = [col[:max_length] for col in data_other.columns]

data_other = data_other[['indvteam', 'descrole']]

sas_data = sas.df2sd(data_other)

c = sas.submitLST("""
proc freq data = work._df;
  table descrole / missing;
run;
""")
```

```{python}
#| eval: false
tables.freq_prop(df[df['Other'] == "Other"], 'descrole')
```

<!-- ## Program Goal -->
```{python}
#| eval: false
df_truncated_names = df.copy()

df_truncated_names.columns = [col[:max_length] for col in df_truncated_names.columns]

df_truncated_names = df_truncated_names[['proggoal', 'indvteam']]

sas_data = sas.df2sd(df_truncated_names)

c = sas.submitLST("""
proc freq data = work._df;
  table proggoal / missing;
run;
""")
```


<!-- ## Setting Type -->
```{python}
#| eval: false
# Select all that apply
# Community, Clinical, Public health, Other.1
data_settingtype = df[['Community', 'Clinical', 'Public health', 'Other.1']]


sas_data = sas.df2sd(data_settingtype)

c = sas.submitLST("""
proc freq data = work._df;
  table _character_ / missing;
run;
""")
```

```{python}
#| eval: true
# df['Other setting'] = df['Other.1']
# settingtypes = ['Community', 'Clinical', 'Public health', 'Other setting']

# tables.all_apply(df, settingtypes, 'Characteristic', True)
```

<!-- ## Population Description -->
```{python}
#| eval: false
data_popdesc = df[['popdesc']]

sas_data = sas.df2sd(data_popdesc)

c = sas.submitLST("""
proc freq data = work._df;
  table _character_ / missing;
run;
""")
```




<!-- # Variable frequencies and proportions

# Other variables -->

```{python, warning = FALSE}
#| warning: false

# Loading pandas to the sas session usually results in a warning about the
# indexes not getting retained. This code chunk turns off warnings including
# the index warning
# import warnings
# warnings.filterwarnings('ignore')

# Load the pandas df to the sas session
sas_data = sas.df2sd(df[prefixes], verbose = False)
```



<!-- ## List of all collapsed variables -->
```{python}
# Gives info in each column
# All of the collapsed variables are on the same scale of 0 to 6
# sas_data.columnInfo()
```

<!-- # Display summary measures for each collapsed variable -->
```{python}
#| eval: false
# .describe on each numeric value will give means, sd, min, p25, p50, p75, max
sas_data.describe()

# from great_tables import GT, md, html
# GT(described)
```

## Summary measures and correlation matrix (Spearman's rank)
```{python}
c = sas.submitLST("""
proc corr data = work._df spearman;
run;
""")
```

## Correlogram
```{python}
#| eval: true
corr_matrix = df[prefixes].corr(method = 'spearman')
```

```{python}
# Too many variables to plot and digest, also the values are ordinal from 0-6
# sns.pairplot(df[prefixes])
# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(corr_matrix, 
  annot=False, 
  cmap = cmap, 
  center = .5)
plt.show()

# Add a layer where a p value of less than 0.05 will be marked with a dot
```